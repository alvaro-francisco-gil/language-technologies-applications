{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question-Answer Validation: Machine Learning vs Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alvaro-francisco-gil/nn4nlp/blob/main/exercises/04_transformers_and_contextuals.ipynb) \n",
        "[![View on GitHub](https://img.shields.io/badge/Open%20on-GitHub-blue?logo=github)](https://github.com/alvaro-francisco-gil/nn4nlp/blob/main/exercises/04_transformers_and_contextuals.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The task of this experiment is to measure how much of the truthfulness of a question-answer pair can be inferred using both machine learning and deep learning approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook in Google Colab, you can install the required packages by running the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch transformers pandas numpy seaborn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ld2bWB62Wm",
        "outputId": "b55083a9-4d72-4d37-d172-835a30e2b6ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    XLNetForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available splits: dict_keys(['train', 'validation'])\n",
            "Training set size: (9427, 3)\n",
            "Validation set size: (3270, 3)\n",
            "\n",
            "Example data point:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'do iran and afghanistan speak the same language',\n",
              " 'answer': True,\n",
              " 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boolq = load_dataset(\"boolq\")\n",
        "\n",
        "print(f\"Available splits: {boolq.keys()}\")\n",
        "print(f\"Training set size: {boolq['train'].shape}\")\n",
        "print(f\"Validation set size: {boolq['validation'].shape}\")\n",
        "\n",
        "print(\"\\nExample data point:\")\n",
        "boolq[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this task, we chose the dataset boolq, which is a dataset of boolean questions and answers. As our aim is not directly answering the question, but evaluating wether the answer of a question is correct, we first need to transform the dataset into a question-answer format. For this, we divide each datapoint into two, the correct question-answer pair and the incorrect one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training examples:\n",
            "{'text': \"The question is 'do iran and afghanistan speak the same language' and the answer is 'True'\", 'label': 'correct'}\n",
            "{'text': \"The question is 'do iran and afghanistan speak the same language' and the answer is 'False'\", 'label': 'incorrect'}\n",
            "\n",
            "Validation examples:\n",
            "{'text': \"The question is 'does ethanol take more energy make that produces' and the answer is 'False'\", 'label': 'correct'}\n",
            "{'text': \"The question is 'does ethanol take more energy make that produces' and the answer is 'True'\", 'label': 'incorrect'}\n"
          ]
        }
      ],
      "source": [
        "# Create new training and validation datasets with correct and incorrect question-answer pairs\n",
        "new_train_data = []\n",
        "new_validation_data = []\n",
        "\n",
        "# Process training data\n",
        "for example in boolq[\"train\"]:\n",
        "    # Create correct pair\n",
        "    correct_pair = {\n",
        "        \"text\": f\"The question is '{example['question']}' and the answer is '{example['answer']}'\",\n",
        "        \"label\": \"correct\"\n",
        "    }\n",
        "    new_train_data.append(correct_pair)\n",
        "    \n",
        "    # Create incorrect pair\n",
        "    incorrect_pair = {\n",
        "        \"text\": f\"The question is '{example['question']}' and the answer is '{not example['answer']}'\",\n",
        "        \"label\": \"incorrect\"\n",
        "    }\n",
        "    new_train_data.append(incorrect_pair)\n",
        "\n",
        "# Process validation data\n",
        "for example in boolq[\"validation\"]:\n",
        "    # Create correct pair\n",
        "    correct_pair = {\n",
        "        \"text\": f\"The question is '{example['question']}' and the answer is '{example['answer']}'\",\n",
        "        \"label\": \"correct\"\n",
        "    }\n",
        "    new_validation_data.append(correct_pair)\n",
        "    \n",
        "    # Create incorrect pair\n",
        "    incorrect_pair = {\n",
        "        \"text\": f\"The question is '{example['question']}' and the answer is '{not example['answer']}'\",\n",
        "        \"label\": \"incorrect\"\n",
        "    }\n",
        "    new_validation_data.append(incorrect_pair)\n",
        "\n",
        "print(\"Training examples:\")\n",
        "print(new_train_data[0])\n",
        "print(new_train_data[1])\n",
        "print(\"\\nValidation examples:\")\n",
        "print(new_validation_data[0])\n",
        "print(new_validation_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18854"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By composition, the dataset will be perfectly balanced, with 50% of the data being correct and 50% being incorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Separate x and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The question is 'do iran and afghanistan speak the same language' and the answer is 'True'\n",
            "correct\n"
          ]
        }
      ],
      "source": [
        "X_train = [example['text'] for example in new_train_data]\n",
        "y_train = [example['label'] for example in new_train_data]\n",
        "\n",
        "X_validation = [example['text'] for example in new_validation_data]\n",
        "y_validation = [example['label'] for example in new_validation_data]\n",
        "\n",
        "print(X_train[0])\n",
        "print(y_train[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's encode the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "y_train = [1 if label == 'correct' else 0 for label in y_train]\n",
        "y_validation = [1 if label == 'correct' else 0 for label in y_validation]\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_validation[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question: What should we do with stopwords and numbers?\n",
        "\n",
        "Experiment: Look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The question is 'will there be a season 4 of da vinci's demons' and the answer is 'False'\n",
            "The question is 'will there be a green lantern 2 movie' and the answer is 'True'\n",
            "The question is 'is it legal to own an ar15 in california' and the answer is 'False'\n"
          ]
        }
      ],
      "source": [
        "# Get X_train data containing numbers\n",
        "X_train_with_numbers = [text for text in X_train if any(c.isdigit() for c in text)]\n",
        "print(X_train_with_numbers[0])\n",
        "print(X_train_with_numbers[3])\n",
        "print(X_train_with_numbers[10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems that the numbers are meaningful to be able to answer the questions, so we should not remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The question is 'do iran and afghanistan speak the same language' and the answer is 'True'\n",
            "question 'do iran afghanistan speak language' answer 'True'\n"
          ]
        }
      ],
      "source": [
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Split text into words\n",
        "    words = text.split()\n",
        "    # Filter out stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply stopword removal to the data\n",
        "X_train_without_stopwords = [remove_stopwords(text) for text in X_train]\n",
        "X_validation_without_stopwords = [remove_stopwords(text) for text in X_validation]\n",
        "\n",
        "print(X_train[0])\n",
        "print(X_train_without_stopwords[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the word \"same\" is being removed, so we should not remove the full list of stopwords, as they can contain valuable information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion: The preprocessing should be: Downcase, remove punctuation, and remove stopwords that are not meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question do iran afghanistan speak same language answer true\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Remove only common stopwords that are less meaningful\n",
        "    common_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were'}\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in common_stopwords]\n",
        "    \n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply preprocessing to the data\n",
        "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
        "X_validation_preprocessed = [preprocess_text(text) for text in X_validation]\n",
        "\n",
        "print(X_train_preprocessed[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the semantical meaning of the words is important, we will use word2vec to encode the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (18854, 100)\n",
            "Validation data shape: (6540, 100)\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text data\n",
        "X_train_tokens = [text.split() for text in X_train_preprocessed]\n",
        "X_validation_tokens = [text.split() for text in X_validation_preprocessed]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=X_train_tokens,\n",
        "    vector_size=100,  # Size of word vectors\n",
        "    window=5,         # Context window size\n",
        "    min_count=2,      # Minimum word frequency\n",
        "    workers=4,        # Number of worker threads\n",
        "    sg=1             # Skip-gram model\n",
        ")\n",
        "\n",
        "# Function to get document vector by averaging word vectors\n",
        "def get_document_vector(tokens, model):\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv:\n",
        "            vectors.append(model.wv[token])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Convert documents to vectors\n",
        "X_train_encoded = np.array([get_document_vector(doc, word2vec_model) for doc in X_train_tokens])\n",
        "X_validation_encoded = np.array([get_document_vector(doc, word2vec_model) for doc in X_validation_tokens])\n",
        "\n",
        "print(f\"Training data shape: {X_train_encoded.shape}\")\n",
        "print(f\"Validation data shape: {X_validation_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(X_train, y_train, X_val, y_val):\n",
        "    # Define models and their parameter grids\n",
        "    models = {\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestClassifier(random_state=42),\n",
        "            'params': {\n",
        "                'n_estimators': [100, 200],\n",
        "                'max_depth': [None, 10]\n",
        "            }\n",
        "        },\n",
        "        'SVM': {\n",
        "            'model': SVC(random_state=42),\n",
        "            'params': {\n",
        "                'C': [0.1, 1],\n",
        "                'kernel': ['linear', 'rbf']\n",
        "            }\n",
        "        },\n",
        "        'Logistic Regression': {\n",
        "            'model': LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'),\n",
        "            'params': {\n",
        "                'C': [0.1, 1],\n",
        "                'penalty': ['l1', 'l2']\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Perform grid search for each model\n",
        "    best_models = {}\n",
        "    results = []\n",
        "\n",
        "    for model_name, model_info in models.items():\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model_info['model'],\n",
        "            param_grid=model_info['params'],\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[model_name] = grid_search.best_estimator_\n",
        "        \n",
        "        # Get predictions and metrics\n",
        "        y_pred = grid_search.predict(X_val)\n",
        "        val_accuracy = accuracy_score(y_val, y_pred)\n",
        "        report = classification_report(y_val, y_pred, output_dict=True)\n",
        "        \n",
        "        # Store results for visualization\n",
        "        results.append({\n",
        "            'model': model_name,\n",
        "            'best_params': grid_search.best_params_,\n",
        "            'best_score': grid_search.best_score_,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'precision': report['weighted avg']['precision'],\n",
        "            'recall': report['weighted avg']['recall'],\n",
        "            'f1': report['weighted avg']['f1-score']\n",
        "        })\n",
        "\n",
        "    # Convert results to DataFrame for visualization\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return best_models, results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Random Forest...\n"
          ]
        }
      ],
      "source": [
        "best_models, results_df = train_and_evaluate_models(X_train_encoded, y_train, X_validation_encoded, y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_model_results(results_df):\n",
        "    \"\"\"\n",
        "    Visualize model comparison results and print best model metrics.\n",
        "    \n",
        "    Args:\n",
        "        results_df (pd.DataFrame): DataFrame containing model results\n",
        "    \"\"\"\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Model Comparison\n",
        "    plt.subplot(1, 3, 1)\n",
        "    ax1 = sns.barplot(data=results_df, x='model', y='val_accuracy')\n",
        "    plt.title('Model Validation Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0.5, 0.65)\n",
        "    # Add value labels on top of bars\n",
        "    for p in ax1.patches:\n",
        "        ax1.annotate(f'{p.get_height():.3f}', \n",
        "                    (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "    # Plot 2: Precision and Recall\n",
        "    plt.subplot(1, 3, 2)\n",
        "    metrics_df = results_df.melt(id_vars=['model'], \n",
        "                               value_vars=['precision', 'recall'],\n",
        "                               var_name='metric', value_name='score')\n",
        "    ax2 = sns.barplot(data=metrics_df, x='model', y='score', hue='metric')\n",
        "    plt.title('Precision and Recall by Model')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    # Add value labels on top of bars\n",
        "    for p in ax2.patches:\n",
        "        ax2.annotate(f'{p.get_height():.2f}', \n",
        "                    (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "    # Plot 3: F1 Scores\n",
        "    plt.subplot(1, 3, 3)\n",
        "    ax3 = sns.barplot(data=results_df, x='model', y='f1')\n",
        "    plt.title('F1 Scores by Model')\n",
        "    plt.xticks(rotation=45)\n",
        "    # Add value labels on top of bars\n",
        "    for p in ax3.patches:\n",
        "        ax3.annotate(f'{p.get_height():.2f}', \n",
        "                    (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find the best performing model\n",
        "    best_model = results_df.loc[results_df['val_accuracy'].idxmax()]\n",
        "    print(f\"\\nBest overall model: {best_model['model']}\")\n",
        "    print(f\"Best model accuracy: {best_model['val_accuracy']:.4f}\")\n",
        "    print(f\"Best model precision: {best_model['precision']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_model_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results of 0.6 accuracy are relatively low, given that the baseline is 0.5 accuracy. However, they are still higher than one might expect—especially when we reflect on the nature of the task we are trying to solve with this approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In essence, this task is a general knowledge evaluation. To determine whether a question–answer pair is correct, one would typically need access to factual knowledge. While this kind of evaluation is common for general pretrained models, which have been exposed to large corpora, it's more surprising for models trained only on limited labeled datasets.\n",
        "\n",
        "What makes this setup intriguing is that models still perform better than random guessing on factual correctness judgments, even without explicit external knowledge. There are a couple of strong hypotheses for why this might happen:\n",
        "\n",
        "1. **Implicit Knowledge in Word Embeddings**: Pretrained embeddings like Word2Vec can encode rich semantic relationships based on co-occurrence statistics in large corpora. This allows models to make informed judgments based on the geometry of the embeddings, even without direct supervision on the facts.\n",
        "\n",
        "2. **Data Leakage or Memorization**: The dataset may inadvertently allow information to leak from the training to the validation/test splits. If similar questions, phrasings, or facts are repeated, the model may learn to associate certain patterns with correctness labels.\n",
        "\n",
        "While it’s sometimes suggested that models might exploit superficial statistical cues (e.g., answers that “sound more right”), the specific format of this task—where both the correct and incorrect answers to a question are given in similar wording—makes this unlikely. The model isn't deciding between alternatives based on plausibility; it's classifying whether a specific question–answer pair is factually correct, which requires deeper reasoning or memorization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Approach with BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before passing to the pretrained models, let's explore how much could we get out of the ml models, now using the BERT tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, no preprocessing is required since the BERT tokenizer will handle the preprocessing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize BERT tokenizer\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize text data and convert to numpy arrays for ML models\n",
        "def tokenize_bert_for_ml(texts):\n",
        "    encodings = bert_tokenizer(\n",
        "        texts, \n",
        "        truncation=True, \n",
        "        padding='max_length', \n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    # Convert to numpy arrays and combine input_ids and attention_mask\n",
        "    input_ids = encodings['input_ids'].numpy()\n",
        "    attention_mask = encodings['attention_mask'].numpy()\n",
        "    # Combine features into a single array\n",
        "    return np.concatenate([input_ids, attention_mask], axis=1)\n",
        "\n",
        "# Tokenize training and validation data\n",
        "X_train_bert = tokenize_bert_for_ml(X_train)\n",
        "X_val_bert = tokenize_bert_for_ml(X_validation)\n",
        "\n",
        "print(X_train_bert.shape)\n",
        "print(X_val_bert.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Random Forest...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Training SVM...\n"
          ]
        }
      ],
      "source": [
        "best_models, results_df = train_and_evaluate_models(X_train_bert, y_train, X_val_bert, y_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_model_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Learning Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_bert(X_train, y_train, X_validation, y_validation, epochs=10, batch_size=16, train_bert=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate a BERT model for sequence classification.\n",
        "    \n",
        "    Args:\n",
        "        X_train (list): Training text data\n",
        "        y_train (array): Training labels\n",
        "        X_validation (list): Validation text data\n",
        "        y_validation (array): Validation labels\n",
        "        epochs (int): Number of training epochs\n",
        "        batch_size (int): Batch size for training\n",
        "        train_bert (bool): Whether to train the BERT base model or just the classification head\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (model, training_stats)\n",
        "    \"\"\"\n",
        "    # Initialize BERT tokenizer\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    # Encode the text data\n",
        "    train_encodings = bert_tokenizer(\n",
        "        X_train.tolist(),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = bert_tokenizer(\n",
        "        X_validation.tolist(),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create PyTorch dataset class\n",
        "    class QADataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = QADataset(train_encodings, y_train)\n",
        "    val_dataset = QADataset(val_encodings, y_validation)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize the model\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "\n",
        "    # Freeze BERT parameters if not training the full model\n",
        "    if not train_bert:\n",
        "        for param in model.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    training_stats = []\n",
        "    total_training_time = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_start_time = time.time()\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        train_time = time.time() - train_start_time\n",
        "        total_training_time += train_time\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                total_val_loss += loss.item()\n",
        "                \n",
        "                # Get predictions\n",
        "                logits = outputs.logits\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                val_predictions.extend(predictions.cpu().numpy())\n",
        "                val_true_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "        val_f1 = f1_score(val_true_labels, val_predictions)\n",
        "\n",
        "        # Store statistics\n",
        "        epoch_stats = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'val_f1': val_f1,\n",
        "            'train_time': train_time\n",
        "        }\n",
        "        training_stats.append(epoch_stats)\n",
        "\n",
        "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "        print(f'Validation F1: {val_f1:.4f}')\n",
        "        print(f'Training Time: {train_time:.2f}s')\n",
        "\n",
        "    print(f'\\nTotal Training Time: {total_training_time:.2f}s')\n",
        "    \n",
        "    return model, training_stats\n",
        "\n",
        "model, training_stats = train_and_evaluate_bert(X_train, y_train, X_validation, y_validation, train_bert=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thanks for reading the code, connect with me on [LinkedIn](https://www.linkedin.com/in/alvaro-francisco-gil/) or [GitHub](https://github.com/alvaro-francisco-gil) if you have any questions or comments.\n",
        "\n",
        "*Álvaro Francisco Gil*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
